{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdb13ad7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-17T18:20:50.643830Z",
     "start_time": "2023-04-17T18:20:49.745008Z"
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b15e5c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T08:52:39.297346Z",
     "start_time": "2023-05-16T08:52:39.284915Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "import joblib\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflowjs as tfjs\n",
    "\n",
    "from tensorflow.keras import layers, models, optimizers, regularizers\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.backend import expand_dims\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0b00ca",
   "metadata": {},
   "source": [
    "# BoundingBox Packer Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec7e9243",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T08:52:42.343907Z",
     "start_time": "2023-05-16T08:52:42.300981Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class BinPackerNode:\n",
    "    def __init__(self, x=0, y=0, width=0,height=0, data=None, left=None,right=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.data = data\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def split(self, data, width, height):\n",
    "        self.data = data\n",
    "        self.left = BinPackerNode(self.x,self.y+height, self.width, self.height-height)\n",
    "        self.right = BinPackerNode(self.x+width,self.y, self.width-width, height)\n",
    "        return self\n",
    "    \n",
    "    @staticmethod\n",
    "    def find(node, width, height):\n",
    "        if node.data:\n",
    "            return BinPackerNode.find(node.right, width, height) or BinPackerNode.find(node.left, width, height)\n",
    "        elif width <= node.width and height <= node.height:\n",
    "            return node\n",
    "        return None\n",
    "\n",
    "\n",
    "class BinPacker:\n",
    "    def __init__(self, width, height):\n",
    "        self.root = BinPackerNode(0,0,width,height)\n",
    "    \n",
    "    cbsort = {\n",
    "        \"w\": (lambda a,b: b[\"width\"] - a[\"width\"]),\n",
    "        \"h\": (lambda a,b: b[\"height\"] - a[\"height\"]),\n",
    "        \"a\": (lambda a,b: b[\"width\"]*b[\"height\"] - a[\"width\"]*a[\"height\"]),\n",
    "        \"max\": (lambda a,b: max(b[\"width\"], b[\"height\"]) - max(a[\"width\"], a[\"height\"])),\n",
    "        \"min\": (lambda a,b: min(b[\"width\"], b[\"height\"]) - min(a[\"width\"], a[\"height\"])),\n",
    "        \"random\": (lambda a,b: random.random() - 0.5),\n",
    "        \"height\": (lambda a,b: BinPacker.msort(a, b, ['h','w'])),\n",
    "        \"width\": (lambda a,b: BinPacker.msort(a, b, ['w','h'])),\n",
    "        \"area\": (lambda a,b: BinPacker.msort(a, b, ['a','h','w'])),\n",
    "        \"maxside\": (lambda a,b: BinPacker.msort(a, b, ['max','min','h','w'])),\n",
    "    }\n",
    "    \n",
    "    @staticmethod\n",
    "    def msort(a, b, criteria):\n",
    "        diff = 0\n",
    "        for n in range(len(criteria)):\n",
    "            diff = BinPacker.cbsort[criteria[n]](a,b)\n",
    "            if diff != 0:\n",
    "                break\n",
    "        return diff\n",
    "    \n",
    "    @staticmethod\n",
    "    def swap(a,i,j):\n",
    "        t = a[i]\n",
    "        a[i] = a[j]\n",
    "        a[j] = t\n",
    "\n",
    "    @staticmethod\n",
    "    def sort(arr, criteria = ['height']):\n",
    "        for i in range(0, len(arr)-1):\n",
    "            for j in range(i+1, len(arr)):\n",
    "                if BinPacker.msort(arr[i], arr[j], criteria) > 0:\n",
    "                    BinPacker.swap(arr,i,j)\n",
    "\n",
    "    def fit(self, blocks_src, criteria = ['height']):\n",
    "        res = []\n",
    "        blocks = []\n",
    "        \n",
    "        for i in range(len(blocks_src)):\n",
    "            blocks.append(blocks_src[i])\n",
    "\n",
    "        BinPacker.sort(blocks, criteria)\n",
    "\n",
    "        for i in range(len(blocks)):\n",
    "            block = blocks[i]\n",
    "            w = block[\"width\"]\n",
    "            h = block[\"height\"]\n",
    "            node = BinPackerNode.find(self.root, w,h)\n",
    "            if not node:\n",
    "                continue\n",
    "            if not node.split(block[\"data\"] if \"data\" in block else \"empty\", w,h):\n",
    "                continue\n",
    "            node.width = w\n",
    "            node.height = h\n",
    "            res.append(node)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f216656",
   "metadata": {},
   "source": [
    "## Exemple:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aed5a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T16:33:55.763181Z",
     "start_time": "2023-05-12T16:33:55.741233Z"
    }
   },
   "outputs": [],
   "source": [
    "blocks = [\n",
    "    { \"width\": 100, \"height\": 100, \"data\": {\"color\":0xff0000} },\n",
    "    { \"width\": 100, \"height\": 100, \"data\": {\"color\":0x0000ff} },\n",
    "    { \"width\":  80, \"height\":  80 },\n",
    "    { \"width\":  80, \"height\":  80, \"data\": {\"color\":0x0} },\n",
    "]\n",
    "packer = BinPacker(300,300)\n",
    "res = packer.fit(blocks, [\"area\"])\n",
    "\n",
    "for i in range(len(res)):\n",
    "    node = res[i]\n",
    "    if node.data == \"empty\":\n",
    "        \n",
    "        continue\n",
    "    color = node.data[\"color\"]\n",
    "    print(node.x, node.y, node.width, node.height, color)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f68fdb",
   "metadata": {},
   "source": [
    "# Generic Class for multiclass objects detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3345e4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T08:52:46.891727Z",
     "start_time": "2023-05-16T08:52:46.599940Z"
    }
   },
   "outputs": [],
   "source": [
    "class ObjectDetectionGenerator:\n",
    "    def __init__(self, X_train, y_train, X_test, y_test, dir_path, labels=[],\n",
    "                 layers_width=300, layers_height=300,\n",
    "                 random_state=1, test_size=0.25):\n",
    "        self.random_state = random_state\n",
    "        self.layers_width = layers_width\n",
    "        self.layers_height = layers_height\n",
    "        self.labels = labels\n",
    "        self.sprites = {\n",
    "            \"X_train\": X_train,\n",
    "            \"y_train_id\": y_train,\n",
    "            \"y_train_bbox\": [],\n",
    "            \n",
    "            \"X_val\": [],\n",
    "            \"y_val_id\": [],\n",
    "            \"y_val_bbox\": [],\n",
    "            \n",
    "            \"X_test\": X_test,\n",
    "            \"y_test_id\": y_test,\n",
    "            \"y_test_bbox\": []\n",
    "        }\n",
    "        self.layers = { # sprites composition\n",
    "            \"X_train\": np.array([]),\n",
    "            \"y_train_id\": np.array([]),\n",
    "            \"y_train_bbox\": np.array([]),\n",
    "            \n",
    "            \"X_val\": np.array([]),\n",
    "            \"y_val_id\": np.array([]),\n",
    "            \"y_val_bbox\": np.array([]),\n",
    "            \n",
    "            \"X_test\": np.array([]),\n",
    "            \"y_test_id\": np.array([]),\n",
    "            \"y_test_bbox\": np.array([]),\n",
    "        }\n",
    "        self.dir_path = dir_path\n",
    "        self.dir_path_layers = os.path.join(self.dir_path, 'layers')\n",
    "        \n",
    "        s = self.sprites\n",
    "        s[\"X_train\"],s[\"X_val\"],s[\"y_train_id\"],s[\"y_val_id\"] = train_test_split(X_train, y_train,\n",
    "                                                  test_size=test_size,\n",
    "                                                  random_state=random_state)\n",
    "        \n",
    "    css_colors = {\n",
    "        \"AliceBlue\": \"F0F8FF\",\n",
    "        \"AntiqueWhite\": \"FAEBD7\",\n",
    "        \"Aqua\": \"00FFFF\",\n",
    "        \"Aquamarine\": \"7FFFD4\",\n",
    "        \"Azure\": \"F0FFFF\",\n",
    "        \"Beige\": \"F5F5DC\",\n",
    "        \"Bisque\": \"FFE4C4\",\n",
    "        \"Black\": \"000000\",\n",
    "        \"BlanchedAlmond\": \"FFEBCD\",\n",
    "        \"Blue\": \"0000FF\",\n",
    "        \"BlueViolet\": \"8A2BE2\",\n",
    "        \"Brown\": \"A52A2A\",\n",
    "        \"BurlyWood\": \"DEB887\",\n",
    "        \"CadetBlue\": \"5F9EA0\",\n",
    "        \"Chartreuse\": \"7FFF00\",\n",
    "        \"Chocolate\": \"D2691E\",\n",
    "        \"Coral\": \"FF7F50\",\n",
    "        \"CornflowerBlue\": \"6495ED\",\n",
    "        \"Cornsilk\": \"FFF8DC\",\n",
    "        \"Crimson\": \"DC143C\",\n",
    "        \"Cyan\": \"00FFFF\",\n",
    "        \"DarkBlue\": \"00008B\",\n",
    "        \"DarkCyan\": \"008B8B\",\n",
    "        \"DarkGoldenRod\": \"B8860B\",\n",
    "        \"DarkGray\": \"A9A9A9\",\n",
    "        \"DarkGreen\": \"006400\",\n",
    "        \"DarkKhaki\": \"BDB76B\",\n",
    "        \"DarkMagenta\": \"8B008B\",\n",
    "        \"DarkOliveGreen\": \"556B2F\",\n",
    "        \"Darkorange\": \"FF8C00\",\n",
    "        \"DarkOrchid\": \"9932CC\",\n",
    "        \"DarkRed\": \"8B0000\",\n",
    "        \"DarkSalmon\": \"E9967A\",\n",
    "        \"DarkSeaGreen\": \"8FBC8F\",\n",
    "        \"DarkSlateBlue\": \"483D8B\",\n",
    "        \"DarkSlateGray\": \"2F4F4F\",\n",
    "        \"DarkTurquoise\": \"00CED1\",\n",
    "        \"DarkViolet\": \"9400D3\",\n",
    "        \"DeepPink\": \"FF1493\",\n",
    "        \"DeepSkyBlue\": \"00BFFF\",\n",
    "        \"DimGray\": \"696969\",\n",
    "        \"DodgerBlue\": \"1E90FF\",\n",
    "        \"FireBrick\": \"B22222\",\n",
    "        \"FloralWhite\": \"FFFAF0\",\n",
    "        \"ForestGreen\": \"228B22\",\n",
    "        \"Fuchsia\": \"FF00FF\",\n",
    "        \"Gainsboro\": \"DCDCDC\",\n",
    "        \"GhostWhite\": \"F8F8FF\",\n",
    "        \"Gold\": \"FFD700\",\n",
    "        \"GoldenRod\": \"DAA520\",\n",
    "        \"Gray\": \"808080\",\n",
    "        \"Green\": \"008000\",\n",
    "        \"GreenYellow\": \"ADFF2F\",\n",
    "        \"HoneyDew\": \"F0FFF0\",\n",
    "        \"HotPink\": \"FF69B4\",\n",
    "        \"IndianRed\": \"CD5C5C\",\n",
    "        \"Indigo\": \"4B0082\",\n",
    "        \"Ivory\": \"FFFFF0\",\n",
    "        \"Khaki\": \"F0E68C\",\n",
    "        \"Lavender\": \"E6E6FA\",\n",
    "        \"LavenderBlush\": \"FFF0F5\",\n",
    "        \"LawnGreen\": \"7CFC00\",\n",
    "        \"LemonChiffon\": \"FFFACD\",\n",
    "        \"LightBlue\": \"ADD8E6\",\n",
    "        \"LightCoral\": \"F08080\",\n",
    "        \"LightCyan\": \"E0FFFF\",\n",
    "        \"LightGoldenRodYellow\": \"FAFAD2\",\n",
    "        \"LightGrey\": \"D3D3D3\",\n",
    "        \"LightGreen\": \"90EE90\",\n",
    "        \"LightPink\": \"FFB6C1\",\n",
    "        \"LightSalmon\": \"FFA07A\",\n",
    "        \"LightSeaGreen\": \"20B2AA\",\n",
    "        \"LightSkyBlue\": \"87CEFA\",\n",
    "        \"LightSlateGray\": \"778899\",\n",
    "        \"LightSteelBlue\": \"B0C4DE\",\n",
    "        \"LightYellow\": \"FFFFE0\",\n",
    "        \"Lime\": \"00FF00\",\n",
    "        \"LimeGreen\": \"32CD32\",\n",
    "        \"Linen\": \"FAF0E6\",\n",
    "        \"Magenta\": \"FF00FF\",\n",
    "        \"Maroon\": \"800000\",\n",
    "        \"MediumAquaMarine\": \"66CDAA\",\n",
    "        \"MediumBlue\": \"0000CD\",\n",
    "        \"MediumOrchid\": \"BA55D3\",\n",
    "        \"MediumPurple\": \"9370D8\",\n",
    "        \"MediumSeaGreen\": \"3CB371\",\n",
    "        \"MediumSlateBlue\": \"7B68EE\",\n",
    "        \"MediumSpringGreen\": \"00FA9A\",\n",
    "        \"MediumTurquoise\": \"48D1CC\",\n",
    "        \"MediumVioletRed\": \"C71585\",\n",
    "        \"MidnightBlue\": \"191970\",\n",
    "        \"MintCream\": \"F5FFFA\",\n",
    "        \"MistyRose\": \"FFE4E1\",\n",
    "        \"Moccasin\": \"FFE4B5\",\n",
    "        \"NavajoWhite\": \"FFDEAD\",\n",
    "        \"Navy\": \"000080\",\n",
    "        \"OldLace\": \"FDF5E6\",\n",
    "        \"Olive\": \"808000\",\n",
    "        \"OliveDrab\": \"6B8E23\",\n",
    "        \"Orange\": \"FFA500\",\n",
    "        \"OrangeRed\": \"FF4500\",\n",
    "        \"Orchid\": \"DA70D6\",\n",
    "        \"PaleGoldenRod\": \"EEE8AA\",\n",
    "        \"PaleGreen\": \"98FB98\",\n",
    "        \"PaleTurquoise\": \"AFEEEE\",\n",
    "        \"PaleVioletRed\": \"D87093\",\n",
    "        \"PapayaWhip\": \"FFEFD5\",\n",
    "        \"PeachPuff\": \"FFDAB9\",\n",
    "        \"Peru\": \"CD853F\",\n",
    "        \"Pink\": \"FFC0CB\",\n",
    "        \"Plum\": \"DDA0DD\",\n",
    "        \"PowderBlue\": \"B0E0E6\",\n",
    "        \"Purple\": \"800080\",\n",
    "        \"Red\": \"FF0000\",\n",
    "        \"RosyBrown\": \"BC8F8F\",\n",
    "        \"RoyalBlue\": \"4169E1\",\n",
    "        \"SaddleBrown\": \"8B4513\",\n",
    "        \"Salmon\": \"FA8072\",\n",
    "        \"SandyBrown\": \"F4A460\",\n",
    "        \"SeaGreen\": \"2E8B57\",\n",
    "        \"SeaShell\": \"FFF5EE\",\n",
    "        \"Sienna\": \"A0522D\",\n",
    "        \"Silver\": \"C0C0C0\",\n",
    "        \"SkyBlue\": \"87CEEB\",\n",
    "        \"SlateBlue\": \"6A5ACD\",\n",
    "        \"SlateGray\": \"708090\",\n",
    "        \"Snow\": \"FFFAFA\",\n",
    "        \"SpringGreen\": \"00FF7F\",\n",
    "        \"SteelBlue\": \"4682B4\",\n",
    "        \"Tan\": \"D2B48C\",\n",
    "        \"Teal\": \"008080\",\n",
    "        \"Thistle\": \"D8BFD8\",\n",
    "        \"Tomato\": \"FF6347\",\n",
    "        \"Turquoise\": \"40E0D0\",\n",
    "        \"Violet\": \"EE82EE\",\n",
    "        \"Wheat\": \"F5DEB3\",\n",
    "        \"White\": \"FFFFFF\",\n",
    "        \"WhiteSmoke\": \"F5F5F5\",\n",
    "        \"Yellow\": \"FFFF00\",\n",
    "        \"YellowGreen\": \"9ACD32\"\n",
    "    }\n",
    "    \n",
    "    @staticmethod\n",
    "    def list_first_val(arr):\n",
    "        for i in arr:\n",
    "            if i > 0:\n",
    "                return 1\n",
    "        return -1\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_bbox(data):\n",
    "        \"\"\"aligned-axis bounding-box (bounding square)\"\"\"\n",
    "        x1 = 0xffff\n",
    "        y1 = 0xffff\n",
    "        x2 = 0\n",
    "        y2 = 0\n",
    "        # y1\n",
    "        for j in range(len(data)):\n",
    "            if ObjectDetectionGenerator.list_first_val(data[j]) > 0:\n",
    "                y1 = j\n",
    "                break\n",
    "        # y2\n",
    "        for j in range(len(data)):\n",
    "            end = len(data)-j-1\n",
    "            if ObjectDetectionGenerator.list_first_val(data[end]) > 0:\n",
    "                y2 = end\n",
    "                break\n",
    "        # x1, x2\n",
    "        for j in range(len(data)):\n",
    "            ydata = data[j]\n",
    "            val = 0xffff\n",
    "            last = 0\n",
    "            for i in range(len(ydata)):\n",
    "                if ydata[i] > 0:\n",
    "                    x1 = min(x1,i)\n",
    "                    x2 = max(x2,i)\n",
    "        return np.array([x1,y1, x2+1,y2+1])\n",
    "\n",
    "    @staticmethod\n",
    "    def get_X_bbox(X):\n",
    "        bboxes = []\n",
    "        for i in range(len(X)):\n",
    "            bboxes.append(ObjectDetectionGenerator.get_bbox(X[i]))\n",
    "            #if i > 10: break\n",
    "        return bboxes\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_bbox_csv(dir_path, filename):\n",
    "        file_path = os.path.join(dir_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        return np.array(list(df.itertuples(index=False, name=None)))\n",
    "\n",
    "    @staticmethod\n",
    "    def write_bbox_csv(dir_path, filename, X):\n",
    "        file_path = os.path.join(dir_path, filename)\n",
    "        bbox = ObjectDetectionGenerator.get_X_bbox(X)\n",
    "        df = pd.DataFrame(bbox, columns=['x1','y1','x2','y2'])\n",
    "        df.to_csv(file_path, index=False)\n",
    "        return bbox\n",
    "\n",
    "    @staticmethod\n",
    "    def scaling2D(X_train, X_val, X_test, div=[1,1]):\n",
    "        mat2_div = np.array([div[0],div[1],div[0],div[1]])\n",
    "        X_train_scaled = X_train / mat2_div\n",
    "        X_val_scaled = X_val / mat2_div\n",
    "        X_test_scaled = X_test / mat2_div\n",
    "        return X_train_scaled, X_val_scaled, X_test_scaled\n",
    "    \n",
    "    @staticmethod\n",
    "    def _draw_sprite(src, sx,sy, s_width,s_height,\n",
    "                     dst, dx,dy, d_width,d_height):\n",
    "        # src\n",
    "        s_width_orig = src.shape[1]\n",
    "        s_height_orig = src.shape[0]\n",
    "        # dst\n",
    "        d_width_orig = dst.shape[1]\n",
    "        d_height_orig = dst.shape[0]\n",
    "        dx -= sx\n",
    "        dy -= sy\n",
    "        for j in range(sy,s_height):\n",
    "            for i in range(sx,s_width):\n",
    "                color = src[j][i]\n",
    "                dst[j+dy][i+dx] = color# if color else 127\n",
    "                \n",
    "    @staticmethod\n",
    "    def _bbox_collide_bbox_list(x,y, bbox, pos_list,bbox_list):\n",
    "        if len(bbox_list) == 0:\n",
    "            return False\n",
    "        a = bbox\n",
    "        for i in range(len(bbox_list)):\n",
    "            x2 = pos_list[i][0]\n",
    "            y2 = pos_list[i][1]\n",
    "            b = bbox_list[i]\n",
    "            if a[2]+x < b[0]+x2 or a[0]+x > b[2]+x2:\n",
    "                continue\n",
    "            if a[3]+y < b[1]+y2 or a[1]+y > b[3]+y2:\n",
    "                continue\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    @staticmethod\n",
    "    def _plt_rectangle(tup, color_id=0, text=\"None\"):\n",
    "        x1 = tup[0]\n",
    "        y1 = tup[1]\n",
    "        x2 = tup[2]\n",
    "        y2 = tup[3]\n",
    "        # body\n",
    "        color = '#' + list(ObjectDetectionGenerator.css_colors.values())[color_id]\n",
    "        rec_body = Rectangle((x1-0.5,y1-0.5),x2-x1,y2-y1,\n",
    "                             linewidth=4,edgecolor=color,facecolor='none')\n",
    "        plt.gca().add_patch(rec_body)\n",
    "        # title\n",
    "        if text != \"None\":\n",
    "            bbox = dict(edgecolor='none', facecolor=color)\n",
    "            plt.text(x1-0.5, y1-1.5, text,  color=\"black\",\n",
    "                     ha=\"left\", va=\"bottom\", fontsize=30, bbox=bbox)\n",
    "        return\n",
    "    \n",
    "    def load_sprites(self, gen_files=True, test_size=0.25, random_state=1):\n",
    "        \"\"\"\n",
    "        - convert csv to png\n",
    "        - load or generate+save aabb's\n",
    "        \"\"\"\n",
    "        s = self.sprites\n",
    "        \n",
    "        if (not gen_files) or self.dir_path == None:\n",
    "            s[\"y_train_bbox\"] = self.get_X_bbox(s[\"X_train\"])\n",
    "            s[\"y_val_bbox\"] = self.get_X_bbox(s[\"X_val\"])\n",
    "            s[\"y_test_bbox\"] = self.get_X_bbox(s[\"X_test\"])\n",
    "            return\n",
    "        # folder exists ?\n",
    "        if not os.path.exists(self.dir_path):\n",
    "            os.mkdir(self.dir_path)\n",
    "        # load aabb\n",
    "        if os.path.exists(os.path.join(self.dir_path, 'y_train_bbox.csv')):\n",
    "            s[\"y_train_bbox\"] = self.read_bbox_csv(self.dir_path, 'y_train_bbox.csv')\n",
    "            s[\"y_val_bbox\"] = self.read_bbox_csv(self.dir_path, 'y_val_bbox.csv')\n",
    "            s[\"y_test_bbox\"] = self.read_bbox_csv(self.dir_path, 'y_test_bbox.csv')\n",
    "            print(\"bbox CSVs loaded\")\n",
    "        else: # or compute them (slow!)\n",
    "            s[\"y_train_bbox\"] = self.write_bbox_csv(self.dir_path, 'y_train_bbox.csv', s[\"X_train\"])\n",
    "            s[\"y_val_bbox\"] = self.write_bbox_csv(self.dir_path, 'y_val_bbox.csv', s[\"X_val\"])\n",
    "            s[\"y_test_bbox\"] = self.write_bbox_csv(self.dir_path, 'y_test_bbox.csv', s[\"X_test\"])\n",
    "            print(\"bbox CSVs computed & saved\")\n",
    "        \n",
    "        \"\"\"s[\"y_train_bbox\"], s[\"y_val_bbox\"] = train_test_split(s[\"y_train_bbox\"],\n",
    "                                                          test_size=test_size,\n",
    "                                                          random_state=random_state)\"\"\"\n",
    "    \n",
    "    def write_layers(self, X_set=\"train\"):\n",
    "        dir_path_layers = os.path.join(self.dir_path_layers, X_set)\n",
    "        X_set = \"X_\" + X_set\n",
    "        images_len = len(self.layers[X_set])\n",
    "        for i in range(images_len):\n",
    "            png_filename = os.path.join(dir_path_layers, f\"{X_set}_{i}.png\")\n",
    "            pixels = np.reshape(self.layers[X_set][i],(self.layers_height,self.layers_width))\n",
    "            img = Image.fromarray(pixels.astype(np.uint8), mode='L')\n",
    "            img.save(png_filename, format='PNG', bits=8)\n",
    "        print(f\"{images_len} layers images written!\")\n",
    "        return self\n",
    "\n",
    "    def read_layers(self, X_set=\"train\"):\n",
    "        dir_path_layers = os.path.join(self.dir_path_layers, X_set)\n",
    "        i = 0\n",
    "        X_set = \"X_\" + X_set\n",
    "        shape = (self.layers_height, self.layers_width, 1)\n",
    "        arr = []\n",
    "        while True:\n",
    "            png_filename = os.path.join(dir_path_layers, f\"{X_set}_{i}.png\")\n",
    "            if not os.path.exists(png_filename):\n",
    "                break\n",
    "            img = Image.open(png_filename)\n",
    "            arr.append(np.reshape(img.getdata(),shape))\n",
    "            i += 1\n",
    "        self.layers[X_set] = np.array(arr)\n",
    "        print(f\"{i} layers images readed!\")\n",
    "        return self\n",
    "\n",
    "    def load_layers(self, X_set=\"train\", count=-1):\n",
    "        dir_path_layers_set = os.path.join(self.dir_path_layers, X_set)\n",
    "\n",
    "        # folders exists ?\n",
    "        if not os.path.exists(dir_path_layers_set):\n",
    "            os.mkdir(dir_path_layers_set)\n",
    "\n",
    "        json_id_filename = f'layers_y_id_{X_set}.json'\n",
    "        json_id_filepath = os.path.join(dir_path_layers_set, json_id_filename)\n",
    "        json_bbox_filename = f'layers_y_bbox_{X_set}.json'\n",
    "        json_bbox_filepath = os.path.join(dir_path_layers_set, json_bbox_filename)\n",
    "\n",
    "        if not os.path.exists(json_id_filepath):\n",
    "            # write json (id's)\n",
    "            json_str = pd.Series(self.layers[f\"y_{X_set}_id\"]).to_json(orient='values')\n",
    "            with open(json_id_filepath, 'w', encoding='utf-8') as f:\n",
    "                f.write(json_str)\n",
    "            # write json (bbox's)\n",
    "            json_str = pd.Series(self.layers[f\"y_{X_set}_bbox\"]).to_json(orient='values')\n",
    "            with open(json_bbox_filepath, 'w', encoding='utf-8') as f:\n",
    "                f.write(json_str)\n",
    "            # write png's\n",
    "            self.write_layers(X_set)\n",
    "            print(f'\"{X_set}\" png and bbox saved!')\n",
    "        else:\n",
    "            self.reset_layer(X_set)\n",
    "            # read json (id's)\n",
    "            arr = pd.read_json(json_id_filepath, typ='series', orient='values', dtype=np.int8)\n",
    "            self.layers[f\"y_{X_set}_id\"] = np.array(list(arr))\n",
    "            # read json (id's)\n",
    "            arr = pd.read_json(json_bbox_filepath, typ='series', orient='values', dtype=np.int16)\n",
    "            self.layers[f\"y_{X_set}_bbox\"] = np.array(list(arr))\n",
    "            # read png's\n",
    "            self.read_layers(X_set)\n",
    "            print(f'\"{X_set}\" png and bbox loaded!')\n",
    "    \n",
    "    def reset_layer(self, X_set):\n",
    "        self.layers[f\"X_{X_set}\"] = np.array([])\n",
    "        self.layers[f\"y_{X_set}_id\"] = np.array([])\n",
    "        self.layers[f\"y_{X_set}_bbox\"] = np.array([])\n",
    "        return self\n",
    "    \n",
    "    def reset_layers(self):\n",
    "        self.reset_layer(\"train\").reset_layer(\"val\").reset_layer(\"test\")\n",
    "        return self\n",
    "    \n",
    "    def clean_layer(self, X_set=\"train\", layer_index=0):\n",
    "        y_set = \"y_\" + X_set\n",
    "        X_set = \"X_\" + X_set\n",
    "        if len(self.layers[X_set]) > layer_index:\n",
    "            shape = (self.layers_height, self.layers_width, 1)\n",
    "            self.layers[X_set][layer_index] = np.zeros(shape)\n",
    "            self.layers[y_set+\"_id\"][layer_index] = np.array([])\n",
    "            self.layers[y_set+\"_bbox\"][layer_index] = np.array([])\n",
    "        return self\n",
    "    \n",
    "    def add_layer(self, X_set=\"train\"):\n",
    "        y_set = \"y_\" + X_set\n",
    "        X_set = \"X_\" + X_set\n",
    "        shape = (self.layers_height, self.layers_width, 1)\n",
    "        self.layers[X_set] = np.append(self.layers[X_set], np.zeros(shape))\n",
    "        self.layers[y_set+\"_id\"] = np.append(self.layers[y_set+\"_id\"], [])\n",
    "        self.layers[y_set+\"_bbox\"] = np.append(self.layers[y_set+\"_bbox\"], [])\n",
    "        return self\n",
    "\n",
    "    def layer_draw_random_sprite(self, X_set=\"train\",\n",
    "                                 layer_index=0, sprite_index=0, x=None, y=None):\n",
    "        y_set = \"y_\" + X_set\n",
    "        X_set = \"X_\" + X_set\n",
    "        dst = self.layers[X_set][layer_index]\n",
    "        src = self.sprites[X_set][sprite_index]\n",
    "        bbox = self.sprites[y_set+'_bbox'][sprite_index]\n",
    "        if x == None:\n",
    "            x = random.randint(0, self.layers_width-bbox[2])\n",
    "        if y == None:\n",
    "            y = random.randint(0, self.layers_height-bbox[3])\n",
    "        bbox_train = np.array([x,y, x+(bbox[2]-bbox[0]), y+(bbox[3]-bbox[1])])\n",
    "        self.layers[y_set+\"_bbox\"][layer_index] = np.append(self.layers[y_set+\"_bbox\"][layer_index],\n",
    "                                                            bbox_train)\n",
    "        self.layers[y_set+\"_id\"][layer_index] = np.append(self.layers[y_set+\"_id\"][layer_index],\n",
    "                                                          self.sprites[y_set+'_id'][sprite_index])\n",
    "        #self.layers[y_set+\"_bbox\"][layer_index].append(bbox_train)\n",
    "        #self.layers[y_set+\"_id\"][layer_index].append(self.sprites[y_set+'_id'][sprite_index])\n",
    "        self._draw_sprite(src,\n",
    "                          bbox[0],bbox[1], bbox[2],bbox[3],\n",
    "                          dst, x,y, dst.shape[0],dst.shape[1])\n",
    "        \n",
    "            \n",
    "    def layer_draw_random_sprites(self, X_set=\"train\", layer_index=0, count=20):\n",
    "        y_set = \"y_\" + X_set\n",
    "        X_set = \"X_\" + X_set\n",
    "        random_call_count = 0\n",
    "        dst = self.layers['X_train'][layer_index]\n",
    "        pos_list = []\n",
    "        bbox_list = []\n",
    "        \n",
    "        for i in range(count):\n",
    "            idx = random.randint(0, len(self.sprites[X_set]))\n",
    "            src = self.sprites[X_set][idx]\n",
    "            bbox = self.sprites[y_set+'_bbox'][idx]\n",
    "            for j in range(200):\n",
    "                x = random.randint(0, self.layers_width-bbox[2])\n",
    "                y = random.randint(0, self.layers_height-bbox[3])\n",
    "                collision = self._bbox_collide_bbox_list(x,y, bbox, pos_list,bbox_list)\n",
    "                if collision == False:\n",
    "                    break\n",
    "                random_call_count += 1\n",
    "                if j >= 199:\n",
    "                    raise NameError('Too many loops! reduce the number of chars')\n",
    "            pos_list.append([x,y])\n",
    "            bbox_list.append(bbox)\n",
    "            self.layer_draw_random_sprite(layer_index, idx, x,y)\n",
    "                \n",
    "            \"\"\"bbox_train = np.array([x,y, x+(bbox[2]-bbox[0]), y+(bbox[3]-bbox[1])])\n",
    "            self.layers[\"y_train_bbox\"][layer_index].append(bbox_train)\n",
    "            self.layers[\"y_train_id\"][layer_index].append(self.sprites['y_train'][idx])\n",
    "            self._draw_sprite(src,\n",
    "                              bbox[0],bbox[1], bbox[2],bbox[3],\n",
    "                              dst, x,y, dst.shape[0],dst.shape[1])\"\"\"\n",
    "\n",
    "    def layer_draw_packed_sprites(self, X_set=\"train\", layer_index=0, count=20):\n",
    "        y_set = \"y_\" + X_set\n",
    "        X_set = \"X_\" + X_set\n",
    "        random_call_count = 0\n",
    "        dst = self.layers['X_train'][layer_index]\n",
    "        blocks = []\n",
    "        x_pad = 2\n",
    "        y_pad = 2\n",
    "        \n",
    "        for i in range(count):\n",
    "            idx = random.randint(0, len(self.sprites[X_set]))\n",
    "            src = self.sprites[X_set][idx]\n",
    "            bbox = self.sprites[y_set+'_bbox'][idx]\n",
    "            blocks.append({\n",
    "                \"width\": (bbox[2]-bbox[0]) + x_pad,\n",
    "                \"height\": (bbox[3]-bbox[1]) + y_pad,\n",
    "                \"data\": {\n",
    "                    \"idx\": idx,\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        # packer\n",
    "        packer = BinPacker(self.layers_width,self.layers_height)\n",
    "        res = packer.fit(blocks, [\"area\"])\n",
    "        \n",
    "        for i in range(len(res)):\n",
    "            node = res[i]\n",
    "            if node.data == \"empty\":\n",
    "                continue\n",
    "            idx = node.data[\"idx\"]\n",
    "            src = self.sprites[X_set][idx]\n",
    "            bbox = self.sprites[y_set+'_bbox'][idx]\n",
    "            self._draw_sprite(src,\n",
    "                              #0,0, 28,28,\n",
    "                              bbox[0],bbox[1], bbox[2],bbox[3],\n",
    "                              dst, node.x,node.y, dst.shape[1],dst.shape[0])\n",
    "    \n",
    "    def transform_scale_sprites(self, scale=[1,1]):\n",
    "        s = self.sprites\n",
    "        return self.scaling2D(s[\"y_train_bbox\"], s[\"y_val_bbox\"], s[\"y_test_bbox\"], scale)\n",
    "    \n",
    "    def transform_scale_layers(self, scale=[1,1]):\n",
    "        s = self.layers\n",
    "        return self.scaling2D(s[\"y_train_bbox\"], s[\"y_val_bbox\"], s[\"y_test_bbox\"], scale)\n",
    "    \n",
    "    \n",
    "    def show_sprite(self, X_set=\"train\", idx=0):\n",
    "        X_set = \"X_\" + X_set\n",
    "        s = self.sprites\n",
    "        plt.imshow(s[X_set][idx], cmap='gray')\n",
    "        plt.show()\n",
    "        \n",
    "    def show_layer(self, X_set=\"train\", idx=0):\n",
    "        y_set = \"y_\" + X_set\n",
    "        X_set = \"X_\" + X_set\n",
    "        s = self.layers\n",
    "        #plt.figure(figsize = (20,int(20*(self.layers_height / self.layers_width))))\n",
    "        plt.figure(figsize = (20,20))\n",
    "        #print(s[\"X_train\"][idx].shape)\n",
    "        #print(s[\"aabb_train\"][idx])\n",
    "        # draw layer\n",
    "        plt.imshow(s[X_set][idx], cmap='gray')\n",
    "        # draw aabb's\n",
    "        text = None\n",
    "        for i in range(len(s[y_set+\"_bbox\"][idx])):\n",
    "            class_id = s[y_set+\"_id\"][idx][i]\n",
    "            color_id = class_id + 10\n",
    "            if len(self.labels) > 0:\n",
    "                text = self.labels[class_id]\n",
    "            self._plt_rectangle(s[y_set+\"_bbox\"][idx][i], color_id, text)\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497309ea",
   "metadata": {},
   "source": [
    "## Exemple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584ebc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"dataset_dir = os.path.join(os.path.dirname(os.getcwd()), 'dataset')\n",
    "arr = pd.read_json(dataset_dir+\"/layers/test/layers_y_bbox_test.json\", orient='values')\n",
    "print(np.array(list(arr[0])))\"\"\"\n",
    "\n",
    "dataset_dir = os.path.join(os.path.dirname(os.getcwd()), 'dataset')\n",
    "arr = pd.read_json(dataset_dir+\"/layers/test/layers_y_bbox_test.json\", typ='series', orient='values')\n",
    "arr = np.array(list(arr))\n",
    "\n",
    "print(arr.shape)\n",
    "print(arr)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "print(np.reshape(arr, (-1,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12673a30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T16:46:35.096074Z",
     "start_time": "2023-05-12T16:42:18.745662Z"
    }
   },
   "outputs": [],
   "source": [
    "try: # python\n",
    "    dataset_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"..\", \"dataset\")\n",
    "except NameError: # jupyter notebook\n",
    "    dataset_dir = os.path.join(os.path.dirname(os.getcwd()), 'dataset')\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "og = ObjectDetectionGenerator(X_train, y_train, X_test, y_test, dataset_dir,\n",
    "                              layers_width=256,layers_height=128,\n",
    "                              labels=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"])\n",
    "og.load_sprites()\n",
    "#og.transform_scale([28.,28.])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c7adc8",
   "metadata": {},
   "source": [
    "## Dataset generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a724ca1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T20:41:06.705603Z",
     "start_time": "2023-05-14T20:40:48.588267Z"
    }
   },
   "outputs": [],
   "source": [
    "try: # python\n",
    "    dataset_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"..\", \"dataset\")\n",
    "except NameError: # jupyter notebook\n",
    "    dataset_dir = os.path.join(os.path.dirname(os.getcwd()), 'dataset')\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "og = ObjectDetectionGenerator(X_train, y_train, X_test, y_test, dataset_dir,\n",
    "                              layers_width=256,layers_height=128,\n",
    "                              labels=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"])\n",
    "og.load_sprites()\n",
    "#og.transform_scale_sprites([28.,28.])\n",
    "\n",
    "print(\"\\ntrain:\")\n",
    "print(og.sprites['X_train'].shape)\n",
    "print(og.sprites['y_train_id'].shape)\n",
    "print(og.sprites['y_train_bbox'].shape)\n",
    "print(\"\\nval:\")\n",
    "print(og.sprites['X_val'].shape)\n",
    "print(og.sprites['y_val_id'].shape)\n",
    "print(og.sprites['y_val_bbox'].shape)\n",
    "print(\"\\ntest:\")\n",
    "print(og.sprites['X_test'].shape)\n",
    "print(og.sprites['y_test_id'].shape)\n",
    "print(og.sprites['y_test_bbox'].shape)\n",
    "\n",
    "div = 10 # 1\n",
    "\n",
    "for i in range(int(len(og.sprites[\"X_train\"])/div)):\n",
    "    og.add_layer(\"train\")\n",
    "    og.layer_draw_random_sprite(\"train\", i, i)\n",
    "og.load_layers(\"train\")\n",
    "og.reset_layers()\n",
    "\n",
    "for i in range(int(len(og.sprites[\"X_val\"])/div)):\n",
    "    og.add_layer(\"val\")\n",
    "    og.layer_draw_random_sprite(\"val\", i, i)\n",
    "og.load_layers(\"val\")\n",
    "og.reset_layers()\n",
    "\n",
    "for i in range(int(len(og.sprites[\"X_test\"])/div)):\n",
    "    og.add_layer(\"test\")\n",
    "    og.layer_draw_random_sprite(\"test\", i, i)\n",
    "og.load_layers(\"test\")\n",
    "og.reset_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6a289d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-16T05:32:00.384Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try: # python\n",
    "    dataset_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"..\", \"dataset\")\n",
    "except NameError: # jupyter notebook\n",
    "    dataset_dir = os.path.join(os.path.dirname(os.getcwd()), 'dataset')\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "og = ObjectDetectionGenerator(X_train, y_train, X_test, y_test, dataset_dir,\n",
    "                              layers_width=256,layers_height=128,\n",
    "                              labels=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"])\n",
    "og.load_sprites()\n",
    "#og.transform_scale([28.,28.])\n",
    "\n",
    "#og.reset_layers()\n",
    "#og.add_layer()\n",
    "#og.layer_draw_random_sprites(0, 2)\n",
    "#og.add_layer()\n",
    "#og.layer_draw_random_sprites(1, 5)\n",
    "\n",
    "\n",
    "    \n",
    "#og.show_layer(0)\n",
    "#og.show_layer(1)\n",
    "\n",
    "#print(og.layers[\"aabb_train\"])\n",
    "#print(pd.Series(og.layers[\"aabb_train\"]).to_json(orient='values'))\n",
    "\n",
    "og.load_layers(\"train\")\n",
    "og.load_layers(\"val\")\n",
    "og.load_layers(\"test\")\n",
    "\n",
    "#og.show_layer(\"test\", 0)\n",
    "#og.show_layer(\"test\", 1)\n",
    "\n",
    "\n",
    "#print(og.layers['X_test'].shape)\n",
    "\"\"\"print(og.layers['y_test_bbox'].shape)\n",
    "print(og.layers['y_test_bbox'][0])\n",
    "print(og.layers['y_test_bbox'])\"\"\"\n",
    "#print(og.layers['y_test_bbox'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21259705",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T05:14:30.220152Z",
     "start_time": "2023-05-16T05:14:30.164058Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\ntrain:\")\n",
    "print(og.layers['X_train'].shape)\n",
    "print(og.layers['y_train_id'].shape)\n",
    "print(og.layers['y_train_bbox'].shape)\n",
    "print(\"\\nval:\")\n",
    "print(og.layers['X_val'].shape)\n",
    "print(og.layers['y_val_id'].shape)\n",
    "print(og.layers['y_val_bbox'].shape)\n",
    "print(\"\\ntest:\")\n",
    "print(og.layers['X_test'].shape)\n",
    "print(og.layers['y_test_id'].shape)\n",
    "print(og.layers['y_test_bbox'].shape)\n",
    "\n",
    "print(\"\\n\")\n",
    "#print(og.layers['X_test'][0])\n",
    "#print(og.layers['y_test_id'][0])\n",
    "#print(og.layers['y_test_bbox'])\n",
    "\n",
    "print(og.layers['y_test_bbox'][0].shape)\n",
    "print(np.squeeze(og.layers['y_test_bbox']))\n",
    "#print(np.expand_dims(og.layers['y_test_bbox'], axis=1))\n",
    "\n",
    "to_categorical(og.layers['y_test_id'], num_classes=10, dtype =\"uint8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91424639",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T08:03:06.182960Z",
     "start_time": "2023-05-09T08:03:05.537298Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "og.clean_layer(0)\n",
    "og._draw_packed_sprites(0, 100)\n",
    "og.show_layer(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505c7bca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T08:03:10.532384Z",
     "start_time": "2023-05-09T08:03:10.489431Z"
    }
   },
   "outputs": [],
   "source": [
    "og.reset_layers()\n",
    "og.add_layer()\n",
    "og._draw_random_sprites(0, 1)\n",
    "og.show_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee56d15",
   "metadata": {},
   "source": [
    "# Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df17193f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T08:57:09.395393Z",
     "start_time": "2023-05-16T08:57:09.343884Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class Trainer():\n",
    "    def __init__(self, canvas_width=300, canvas_height=300):\n",
    "        self.canvas_width = canvas_width\n",
    "        self.canvas_heigth = canvas_height\n",
    "        \n",
    "        self.X_train = None\n",
    "        self.y_train = [None,None]\n",
    "        \n",
    "        self.X_val = None\n",
    "        self.y_val = [None,None]\n",
    "        \n",
    "        self.X_test = None\n",
    "        self.y_test = [None,None]\n",
    "        \n",
    "        \n",
    "    \"\"\"def scaling(self):\n",
    "        self.mean_scaler = 0\n",
    "        self.std_scaler = 255.\n",
    "\n",
    "        self.X_train = (self.X_train - self.mean_scaler) / self.std_scaler\n",
    "        self.X_val = (self.X_val - self.mean_scaler) / self.std_scaler\n",
    "        self.X_test = (self.X_test - self.mean_scaler) / self.std_scaler\n",
    "        return self\"\"\"\n",
    "    \n",
    "    \n",
    "    def get_data(self, random_state=1, test_size=0.25):\n",
    "        \"\"\"(self.X_train, self.y_train[0]), (self.X_test, self.y_test[0]) = datasets.mnist.load_data(path=\"mnist.npz\")\"\"\"\n",
    "        \n",
    "        try: # python\n",
    "            dataset_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"..\", \"dataset\")\n",
    "        except NameError: # jupyter notebook\n",
    "            dataset_dir = os.path.join(os.path.dirname(os.getcwd()), 'dataset')\n",
    "\n",
    "        (X_train, y_train), (X_test, y_test) = datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "        self.og = ObjectDetectionGenerator(X_train, y_train, X_test, y_test, dataset_dir,\n",
    "                                           layers_width=256,layers_height=128,\n",
    "                                           labels=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"])\n",
    "        self.og.load_layers(\"train\")\n",
    "        self.og.load_layers(\"val\")\n",
    "        self.og.load_layers(\"test\")\n",
    "\n",
    "        # add a dim: (28, 28) -> (28, 28, 1)\n",
    "        # np.expand_dims(X_train, axis=-1)\n",
    "        \"\"\"self.X_train = self.X_train.reshape(len(self.X_train), 28,28,1)\n",
    "        self.X_test = self.X_test.reshape(len(self.X_test), 28,28,1)\"\"\"\n",
    "\n",
    "        self.X_train = self.og.layers['X_train']\n",
    "        self.y_train[0] = to_categorical(self.og.layers['y_train_id'],\n",
    "                                         num_classes=10, dtype =\"uint8\")\n",
    "        \n",
    "        self.X_val = self.og.layers['X_val']\n",
    "        self.y_val[0] = to_categorical(self.og.layers['y_val_id'],\n",
    "                                         num_classes=10, dtype =\"uint8\")\n",
    "        \n",
    "        self.X_test = self.og.layers['X_test']\n",
    "        self.y_test[0] = to_categorical(self.og.layers['y_test_id'],\n",
    "                                        num_classes=10, dtype =\"uint8\")\n",
    "\n",
    "        self.y_train[1], self.y_val[1], self.y_test[1] = self.og.transform_scale_layers([128.,256.])\n",
    "        \n",
    "        # split train/validation\n",
    "        \"\"\"self.X_train, self.X_val, self.y_train[0], self.y_val[0] = train_test_split(self.X_train, self.y_train[0],\n",
    "                                                                  test_size=test_size,\n",
    "                                                                  random_state=random_state)\"\"\"\n",
    "        # transform data\n",
    "        #self.scaling()\n",
    "        \n",
    "        # data augmentation + BoundingBoxes\n",
    "        \"\"\"self.og = ObjectDetectionGenerator(self.X_train, self.y_train, self.X_test, self.y_test,\n",
    "                                           os.path.join(os.path.dirname(os.getcwd()), 'dataset'))\n",
    "        self.og.gen_aabb(test_size=test_size, random_state=random_state)\n",
    "        self.y_train[1], self.y_val[1], self.y_test[1] = self.og.transform_scale([28.,28.])\"\"\"\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def set_pipeline(self):\n",
    "        \"\"\" classification + regression with 10+4 outputs \"\"\"\n",
    "        # define two sets of inputs\n",
    "        inputs = layers.Input(shape=self.X_train[0].shape)\n",
    "\n",
    "        # the first branch operates on the first input\n",
    "        a = layers.Conv2D(32, kernel_size=(3,3), activation='relu', padding='same')(inputs)\n",
    "        a = layers.MaxPool2D(pool_size=(2,2))(a)\n",
    "        a = layers.Dropout(0.2)(a)\n",
    "\n",
    "        a = layers.Conv2D(64, kernel_size=(3,3), activation='relu', padding='same')(a)\n",
    "        a = layers.MaxPool2D(pool_size=(2,2))(a)\n",
    "        a = layers.Dropout(0.2)(a)\n",
    "        a = layers.Conv2D(64, kernel_size=(3,3), activation='relu', padding='same')(a)\n",
    "        #a = layers.MaxPool2D(pool_size=(2,2))(a)\n",
    "        a = layers.Flatten()(a)\n",
    "        a = layers.Dense(64, activation='relu')(a)\n",
    "\n",
    "        # output for classification\n",
    "        out_softmax = layers.Dense(10, activation='softmax', name='out_softmax')(a)\n",
    "\n",
    "        # output for regression\n",
    "        out_bbox = layers.Dense(32, activation=\"relu\")(a)\n",
    "        out_bbox = layers.Dense(4, activation='sigmoid', name='out_bbox')(a)\n",
    "\n",
    "        self.model = models.Model(inputs=inputs, outputs=[out_softmax,out_bbox])\n",
    "        return self.model.summary()\n",
    "    \n",
    "    def run(self,\n",
    "            optimizer = optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999),\n",
    "            loss = {'out_softmax': 'categorical_crossentropy',\n",
    "                    'out_sigmoid': 'mse'},\n",
    "            metrics = {'out_softmax': 'accuracy',\n",
    "                       'out_sigmoid': 'mae'},\n",
    "            epochs = 50,\n",
    "            batch_size = 16):\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "        self.metrics = metrics\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # https://distill.pub/2017/momentum/\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/keras/metrics\n",
    "        self.model.compile(optimizer=self.optimizer, \n",
    "                           loss=self.loss,\n",
    "                           metrics=self.metrics)\n",
    "\n",
    "        # early stopping\n",
    "        es = EarlyStopping(patience = 10, restore_best_weights = True)\n",
    "        start_time = timer()\n",
    "        self.history = self.model.fit(self.X_train, self.y_train,\n",
    "                                      validation_data = (self.X_val, self.y_val),\n",
    "                                      batch_size = batch_size,\n",
    "                                      epochs = epochs,\n",
    "                                      callbacks = [es],\n",
    "                                      verbose = 1)\n",
    "\n",
    "        self.training_time = timer() - start_time\n",
    "        print(\"training time:\", self.training_time)\n",
    "        return self\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.eval_res = {\n",
    "            \"train\": self.model.evaluate(self.X_train, self.y_train, batch_size=None, verbose = 1),\n",
    "            \"val\": self.model.evaluate(self.X_val, y_val, batch_size=None, verbose = 1),\n",
    "            \"test\": self.model.evaluate(self.X_test, self.y_test, batch_size=None, verbose = 1)\n",
    "        }\n",
    "        return self.eval_res\n",
    "    \n",
    "    def plot_history(self):\n",
    "        labels = ['loss', *self.metrics.values()]\n",
    "        h = self.history\n",
    "        with plt.style.context('seaborn-deep'):\n",
    "            fig, ax = plt.subplots(1, max(2,len(labels)), figsize=(15, 4))\n",
    "            x_axis = np.arange(len(h[labels[0]]))\n",
    "            for i in range(len(labels)):\n",
    "                l = labels[i]\n",
    "                L = l.capitalize()\n",
    "                ax[i].set_title(l)\n",
    "                ax[i].plot(x_axis, h[l], color=\"blue\", linestyle=\":\", marker=\"X\", label=\"Train \"+L)\n",
    "                ax[i].plot(x_axis, h['val_'+l], color=\"orange\", linestyle=\"-\", marker=\"X\", label=\"Val \"+L)\n",
    "                ax[i].grid(axis=\"x\", linewidth=0.5)\n",
    "                ax[i].grid(axis=\"y\", linewidth=0.5)\n",
    "                ax[i].legend()\n",
    "            plt.show()\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91c2353",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "495a99ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T08:57:55.617411Z",
     "start_time": "2023-05-16T08:57:21.845133Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500 layers images readed!\n",
      "\"train\" png and bbox loaded!\n",
      "1500 layers images readed!\n",
      "\"val\" png and bbox loaded!\n",
      "1000 layers images readed!\n",
      "\"test\" png and bbox loaded!\n",
      "\n",
      "X_train: (4500, 128, 256, 1)\n",
      "X_val: (1500, 128, 256, 1)\n",
      "X_test: (1000, 128, 256, 1)\n",
      "\n",
      "y_train[0]/y_train[1]: (4500, 10) (4500, 1, 4)\n",
      "y_val[0]/y_val[1]: (1500, 10) (1500, 1, 4)\n",
      "y_test[0]/y_test[1]: (1000, 10) (1000, 1, 4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer()\n",
    "trainer.get_data()\n",
    "\n",
    "print(\"\")\n",
    "print(\"X_train:\", trainer.X_train.shape);\n",
    "print(\"X_val:\", trainer.X_val.shape);\n",
    "\n",
    "print(\"X_test:\", trainer.X_test.shape);\n",
    "print(\"\")\n",
    "print(\"y_train[0]/y_train[1]:\", trainer.y_train[0].shape, trainer.y_train[1].shape);\n",
    "print(\"y_val[0]/y_val[1]:\", trainer.y_val[0].shape, trainer.y_val[1].shape);\n",
    "print(\"y_test[0]/y_test[1]:\", trainer.y_test[0].shape, trainer.y_test[1].shape);\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca6898d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T23:45:17.018776Z",
     "start_time": "2023-04-28T23:45:16.981258Z"
    }
   },
   "outputs": [],
   "source": [
    "list(trainer.X_train[0][14])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9135b5fb",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac09dfd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T08:58:00.427628Z",
     "start_time": "2023-05-16T08:58:00.101794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 128, 256, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 128, 256, 32  320         ['input_2[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 64, 128, 32)  0          ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 64, 128, 32)  0           ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 64, 128, 64)  18496       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 32, 64, 64)  0           ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 32, 64, 64)   0           ['max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 32, 64, 64)   36928       ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 131072)       0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 64)           8388672     ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " out_softmax (Dense)            (None, 10)           650         ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " out_bbox (Dense)               (None, 4)            260         ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,445,326\n",
      "Trainable params: 8,445,326\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "trainer.set_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb5c393",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65216156",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d928b7b9",
   "metadata": {},
   "source": [
    "# Draw image + Bounding Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535dcdb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T07:05:58.155892Z",
     "start_time": "2023-04-29T07:05:57.600326Z"
    }
   },
   "outputs": [],
   "source": [
    "def plt_rectangle(tup):\n",
    "    x1 = tup[0]\n",
    "    y1 = tup[1]\n",
    "    x2 = tup[2]\n",
    "    y2 = tup[3]\n",
    "    plt.gca().add_patch(Rectangle((x1-0.5,y1-0.5),x2-x1,y2-y1,linewidth=1,edgecolor='r',facecolor='none'))\n",
    "    return\n",
    "\n",
    "\n",
    "idx = 100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.imshow(trainer.X_train[idx], cmap='gray');\n",
    "\n",
    "plt_rectangle(trainer.y_train[1][idx] * 28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba80a43f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T10:26:26.448537Z",
     "start_time": "2023-04-26T10:26:26.073749Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = [\"Number 0\",\n",
    "          \"Number 1\",\n",
    "          \"Number 2\",\n",
    "          \"Number 3\",\n",
    "          \"Number 4\",\n",
    "          \"Number 5\",\n",
    "          \"Number 6\",\n",
    "          \"Number 7\",\n",
    "          \"Number 8\",\n",
    "          \"Number 9\"]\n",
    "\n",
    "def categorical_to_id(arr_cat, labels=None):\n",
    "    res = -1\n",
    "    for i in range(len(arr_cat)):\n",
    "        if arr_cat[i]:\n",
    "            res = i\n",
    "            break\n",
    "    if labels is None:\n",
    "        return res\n",
    "    return labels[res]\n",
    "\n",
    "item = 21\n",
    "\n",
    "print(\"label is:\", categorical_to_id(y_train_cat[item], labels))\n",
    "plt.imshow(X_train_scaled[item], cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c783083c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T07:44:18.136143Z",
     "start_time": "2023-04-27T07:44:17.775461Z"
    }
   },
   "outputs": [],
   "source": [
    "self.set_pipeline():\n",
    "\n",
    "\n",
    "#model = get_classification_model(X_train_scaled)\n",
    "#model = get_regression_model(X_train_scaled)\n",
    "model = get_model(X_train_scaled)\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0c17b4",
   "metadata": {},
   "source": [
    "# Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b50dc56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T22:11:00.395090Z",
     "start_time": "2023-04-26T21:58:55.539809Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def run(model, X_train,y_train, X_val,y_val,\n",
    "        opt = 'adam',\n",
    "        loss = 'categorical_crossentropy',\n",
    "        metrics = ['accuracy'],\n",
    "        epochs = 50,\n",
    "        batch_size = 16):\n",
    "    \n",
    "    # https://distill.pub/2017/momentum/\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/metrics\n",
    "    model.compile(optimizer=opt, \n",
    "                  loss=loss,\n",
    "                  metrics=metrics)\n",
    "    \n",
    "    # early stopping\n",
    "    es = EarlyStopping(patience = 10, restore_best_weights = True)\n",
    "    \n",
    "    start_time = timer()\n",
    "    history = model.fit(X_train, \n",
    "                        y_train,\n",
    "                        #validation_split = 0.3, # auto val generation\n",
    "                        validation_data = (X_val, y_val),\n",
    "                        batch_size = batch_size,\n",
    "                        epochs = epochs,\n",
    "                        callbacks = [es],\n",
    "                        verbose = 1)\n",
    "    \n",
    "    training_time = timer() - start_time\n",
    "    print(\"training time:\", training_time)\n",
    "    \n",
    "    return history\n",
    "\n",
    "# classification with N classes\n",
    "#history = run(model, X_train_scaled,y_train_cat, X_val_scaled,y_val_cat)\n",
    "\n",
    "# regression\n",
    "\"\"\"history = run(model, X_train_scaled,aabb_train, X_val_scaled,aabb_val,\n",
    "              loss='mse',\n",
    "              metrics=['mae'],\n",
    "              opt=optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999))\"\"\"\n",
    "\n",
    "# classification + regression\n",
    "history = run(model, X_train_scaled,[y_train_cat,y_train_aabb], X_val_scaled,[y_val_cat,y_val_aabb],\n",
    "              loss={'out_softmax': 'categorical_crossentropy',\n",
    "                    'out_bbox': 'mse'},\n",
    "              metrics={'out_softmax': 'accuracy',\n",
    "                       'out_bbox': 'mae'},\n",
    "              opt=optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999))\n",
    "\n",
    "# print(\"aabb_train:\", aabb_train[0]);\n",
    "# print(\"aabb_val:\", aabb_val[0]);\n",
    "# print(\"aabb_test:\", aabb_test[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bc0fde",
   "metadata": {},
   "source": [
    "# Layers & Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dde0ee6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T06:55:16.135576Z",
     "start_time": "2023-04-25T06:55:16.123644Z"
    }
   },
   "outputs": [],
   "source": [
    "layer_1 = model.layers[0]\n",
    "layer_1.weights[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa56d04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T06:55:21.720558Z",
     "start_time": "2023-04-25T06:55:21.475463Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(layer_1.weights[0][:,:,:,15], cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf4b1c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T06:55:24.681824Z",
     "start_time": "2023-04-25T06:55:24.668383Z"
    }
   },
   "outputs": [],
   "source": [
    "batch = X_train_scaled[0:10]\n",
    "activation_1 = layer_1(batch)\n",
    "activation_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5143ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T06:55:31.976084Z",
     "start_time": "2023-04-25T06:55:29.564180Z"
    }
   },
   "outputs": [],
   "source": [
    "# first channel output on the first image\n",
    "fit, axs = plt.subplots(4,4, figsize=(15,6))\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        feature_map = activation_1[0,:,:,i+j]\n",
    "        axs[i,j].imshow(feature_map, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcad921",
   "metadata": {},
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b08348",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T22:11:43.617759Z",
     "start_time": "2023-04-26T22:11:43.602021Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_history(history, labels = [\"loss\"]):\n",
    "    h = history.history\n",
    "    with plt.style.context('seaborn-deep'):\n",
    "        fig, ax = plt.subplots(1, max(2,len(labels)), figsize=(15, 4))\n",
    "        x_axis = np.arange(len(h[labels[0]]))\n",
    "        for i in range(len(labels)):\n",
    "            l = labels[i]\n",
    "            L = l.capitalize()\n",
    "            ax[i].set_title(l)\n",
    "            ax[i].plot(x_axis, h[l], color=\"blue\", linestyle=\":\", marker=\"X\", label=\"Train \"+L)\n",
    "            ax[i].plot(x_axis, h['val_'+l], color=\"orange\", linestyle=\"-\", marker=\"X\", label=\"Val \"+L)\n",
    "            ax[i].grid(axis=\"x\", linewidth=0.5)\n",
    "            ax[i].grid(axis=\"y\", linewidth=0.5)\n",
    "            ax[i].legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02767aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T22:11:48.190276Z",
     "start_time": "2023-04-26T22:11:47.514784Z"
    }
   },
   "outputs": [],
   "source": [
    "#plot_history(history, [\"loss\", \"accuracy\"])\n",
    "plot_history(history, [\"loss\", \"out_softmax_accuracy\", \"out_bbox_mae\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbd18d4",
   "metadata": {},
   "source": [
    "# Baseline Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a109e06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T04:18:29.272395Z",
     "start_time": "2023-04-25T04:18:29.262753Z"
    }
   },
   "source": [
    "Zero Rate Classifier:\n",
    "- https://towardsdatascience.com/calculating-a-baseline-accuracy-for-a-classification-model-a4b342ceb88f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c8c83d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T21:54:11.254634Z",
     "start_time": "2023-04-25T21:54:11.240478Z"
    }
   },
   "outputs": [],
   "source": [
    "distr = pd.Series(y_train).value_counts()\n",
    "distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e60bf6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T04:47:14.116093Z",
     "start_time": "2023-04-25T04:47:14.103815Z"
    }
   },
   "outputs": [],
   "source": [
    "def categorical_baseline(y_train):\n",
    "    distr = pd.Series(y_train).value_counts()\n",
    "    total = len(y_train)\n",
    "    return ((distr / total) ** 2).sum()\n",
    "\n",
    "baseline = categorical_baseline(y_train)\n",
    "\n",
    "print(\"Our baseline is: \" + str(baseline*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77c0549",
   "metadata": {},
   "source": [
    "# Evaluation vs test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31426b08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T22:12:45.509306Z",
     "start_time": "2023-04-26T22:12:24.730543Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(X_train,y_train, X_val,y_val, X_test, y_test):\n",
    "    eval_res = {\n",
    "        \"train\": model.evaluate(X_train, y_train, batch_size=None, verbose = 1)[1],\n",
    "        \"val\": model.evaluate(X_val, y_val, batch_size=None, verbose = 1)[1],\n",
    "        \"test\": model.evaluate(X_test, y_test, batch_size=None, verbose = 1)[1]\n",
    "    }\n",
    "    return eval_res\n",
    "\n",
    "#eval_res = evaluate(X_train_scaled,y_train_cat, X_val_scaled,y_val_cat, X_test_scaled, y_test_cat)\n",
    "#eval_res = evaluate(X_train_scaled,y_train_aabb, X_val_scaled,y_val_aabb, X_test_scaled, y_test_aabb)\n",
    "\n",
    "eval_res = evaluate(X_train_scaled, [y_train_cat,y_train_aabb],\n",
    "                    X_val_scaled, [y_val_cat,y_val_aabb],\n",
    "                    X_test_scaled, [y_test_cat,y_test_aabb])\n",
    "\n",
    "#print(f'The accuracy on the train set is of {eval_res[\"train\"]*100:.2f} %')\n",
    "#print(f'The accuracy on the val set is of {eval_res[\"val\"]*100:.2f} %')\n",
    "#print(f'The accuracy on the test set is of {eval_res[\"test\"]*100:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75477333",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T06:48:10.797219Z",
     "start_time": "2023-04-26T06:48:10.674382Z"
    }
   },
   "outputs": [],
   "source": [
    "model.predict(X_test_scaled)\n",
    "#model.predict(np.array([X_test_scaled[label]]))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c0b29b",
   "metadata": {},
   "source": [
    "# Predict one observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0937e483",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T22:15:51.529521Z",
     "start_time": "2023-04-26T22:15:51.147606Z"
    }
   },
   "outputs": [],
   "source": [
    "label = 133\n",
    "\n",
    "predicted = model.predict(np.array([X_test_scaled[label]]))\n",
    "predicted_class = list(predicted[0][0])\n",
    "predicted_linear = predicted[1][0]\n",
    "\n",
    "#print(\"prediction table:\", predicted)\n",
    "plt.imshow(X_test_scaled[label], cmap='gray')\n",
    "\n",
    "predicted_label = labels[predicted_class.index(max(predicted_class))]\n",
    "print(\"Prediction is =\", predicted_label, f\"({max(predicted_class)*100:.2f}%)\")\n",
    "\n",
    "plt_rectangle(predicted_linear*28)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948b2b3d",
   "metadata": {},
   "source": [
    "# Class Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e578c5d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T06:56:36.549379Z",
     "start_time": "2023-04-25T06:56:36.539127Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Prediction table for '{predicted_label}':\")\n",
    "print(\"\\n\",predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b4aa74",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5b734f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T22:16:30.863982Z",
     "start_time": "2023-04-26T22:16:27.307239Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_csv_id():\n",
    "    dir_path = os.path.join(os.path.dirname(os.getcwd()), 'models')\n",
    "    file_path = os.path.join(dir_path, 'ids.csv')\n",
    "    cur_id = 0\n",
    "    # folder exists ?\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.mkdir(dir_path)\n",
    "    # file exists ?\n",
    "    if not os.path.exists(file_path):\n",
    "        df = pd.DataFrame([0], columns=['id'])\n",
    "        df.to_csv(file_path, index=False)\n",
    "    else:\n",
    "        # csv -> dataframe\n",
    "        df = pd.read_csv(file_path)\n",
    "        cur_id = df[\"id\"].iloc[-1] + 1\n",
    "        with open(file_path, \"a\") as f:\n",
    "            csv.writer(f).writerow([cur_id])\n",
    "            \n",
    "    return {\"cur_id\": cur_id, \"dir_path\": dir_path}\n",
    "\n",
    "\n",
    "def save_model(model):\n",
    "    res = create_csv_id()\n",
    "    dir_path = res[\"dir_path\"]\n",
    "    str_id = str(res[\"cur_id\"]).zfill(4)\n",
    "    base_name = f'model_{str_id}'\n",
    "    \n",
    "    # normal\n",
    "    joblib.dump(model, os.path.join(dir_path, base_name+'.pkl'))\n",
    "    \n",
    "    # for tensorflow.js\n",
    "    tfjs.converters.save_keras_model(model, os.path.join(dir_path, base_name))\n",
    "\n",
    "    # for lite\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    tflite_model = converter.convert()\n",
    "    with open(os.path.join(dir_path, base_name+'.tflite'), 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    return\n",
    "\n",
    "save_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a44f68",
   "metadata": {},
   "source": [
    "# Links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23af298",
   "metadata": {},
   "source": [
    "R-CNN:\n",
    "1. https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/\n",
    "2. https://arxiv.org/pdf/1506.01497.pdf\n",
    "3. https://towardsdatascience.com/understanding-and-implementing-faster-r-cnn-a-step-by-step-guide-11acfff216b0\n",
    "\n",
    "\n",
    "https://github.com/weiliu89/caffe/tree/ssd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e909306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
